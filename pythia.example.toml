# Pythia Configuration
# Copy to ~/.pythia.toml or ./pythia.toml

# General settings
log_level = "info"  # debug, info, warn, error
data_dir = ""       # Default: ~/.pythia

# Indexing configuration
[index]
# Patterns to exclude from indexing
exclude = [
  "**/node_modules/**",
  "**/.git/**",
  "**/vendor/**",
  "**/__pycache__/**",
  "**/dist/**",
  "**/build/**",
  "**/*.min.js",
  "**/*.min.css",
  "**/package-lock.json",
  "**/yarn.lock",
  "**/go.sum",
]

# Patterns to include (empty = all source files)
include = [
  "**/*.go",
  "**/*.py",
  "**/*.js",
  "**/*.ts",
  "**/*.jsx",
  "**/*.tsx",
  "**/*.java",
  "**/*.rs",
  "**/*.c",
  "**/*.cpp",
  "**/*.h",
  "**/*.hpp",
  "**/*.rb",
  "**/*.php",
  "**/*.cs",
  "**/*.md",
  "**/*.yaml",
  "**/*.yml",
  "**/*.json",
  "**/*.toml",
]

max_file_size = 1048576     # 1MB
chunk_size = 512            # Tokens per chunk
chunk_overlap = 50          # Overlap between chunks
batch_size = 100            # Documents per batch
concurrent_files = 4        # Parallel file processing
timeout = "30m"

# MCP server configuration
[mcp]
transport = "stdio"         # stdio or http
host = "127.0.0.1"
port = 8080
max_results = 50
request_timeout = "5m"
enable_resources = true

# Git provider configuration
[git]
cache_dir = ""              # Default: ~/.pythia/git-cache
timeout = "10m"

# GitHub provider
[[git.providers]]
name = "github"
type = "github"
base_url = "https://github.com"
token_env = "GITHUB_TOKEN"  # Read token from environment

# GitLab provider
[[git.providers]]
name = "gitlab"
type = "gitlab"
base_url = "https://gitlab.com"
token_env = "GITLAB_TOKEN"

# Bitbucket provider
[[git.providers]]
name = "bitbucket"
type = "bitbucket"
base_url = "https://bitbucket.org"
token_env = "BITBUCKET_TOKEN"

# Custom Git server example
# [[git.providers]]
# name = "internal"
# type = "generic"
# base_url = "https://git.internal.company.com"
# token_env = "INTERNAL_GIT_TOKEN"

# LEANN vector search configuration
[leann]
python_path = "python3"
model_name = "all-MiniLM-L6-v2"  # Sentence transformer model
cache_dir = ""                   # Default: ~/.pythia/leann-cache
device_type = "cpu"              # cpu or cuda
batch_size = 32
max_tokens = 512
compression = true               # Enable storage compression
pruning_factor = 0.3             # Graph pruning factor (0.0-1.0)
